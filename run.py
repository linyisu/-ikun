#!/usr/bin/env python3
"""
Bç«™è¯„è®ºçˆ¬å–åˆ†æå·¥å…·ä¸»å¯åŠ¨æ–‡ä»¶
"""
import sys
import os
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ¥ æ¬¢è¿ä½¿ç”¨Bç«™è¯„è®ºçˆ¬å–åˆ†æå·¥å…·")
    print("=" * 50)
    
    while True:
        print("\nè¯·é€‰æ‹©åŠŸèƒ½:")
        print("1. ğŸš€ å¯åŠ¨Webç•Œé¢ (Streamlit)")
        print("2. ğŸ“¡ å‘½ä»¤è¡Œçˆ¬å–è¯„è®º")
        print("3. ğŸ“Š æ‰¹é‡è¯äº‘åˆ†æ")
        print("4. ğŸ”§ ç³»ç»Ÿæ£€æŸ¥")
        print("0. é€€å‡º")
        
        choice = input("\nè¯·è¾“å…¥é€‰æ‹© (0-4): ").strip()
        
        if choice == "1":
            start_web_app()
        elif choice == "2":
            start_crawler()
        elif choice == "3":
            batch_analysis()
        elif choice == "4":
            system_check()
        elif choice == "0":
            print("ğŸ‘‹ å†è§ï¼")
            break
        else:
            print("âŒ æ— æ•ˆé€‰æ‹©ï¼Œè¯·é‡æ–°è¾“å…¥")

def start_web_app():
    """å¯åŠ¨Streamlit Webåº”ç”¨"""
    print("ğŸš€ æ­£åœ¨å¯åŠ¨Webç•Œé¢...")
    os.system(f"streamlit run {project_root}/report/app.py")

def start_crawler():
    """å¯åŠ¨å‘½ä»¤è¡Œçˆ¬è™«"""
    print("ğŸ“¡ å¯åŠ¨å‘½ä»¤è¡Œçˆ¬è™«")
    from crawler.bilibili_crawler import main as crawler_main
    crawler_main()

def batch_analysis():
    """æ‰¹é‡è¯äº‘åˆ†æ"""
    print("ğŸ“Š æ‰¹é‡è¯äº‘åˆ†æ")
    from analysis.word_cloud import analyze_comments_from_csv
    from config.settings import DATA_DIR
    
    # æŸ¥æ‰¾CSVæ–‡ä»¶
    csv_files = list(DATA_DIR.glob("*.csv"))
    if not csv_files:
        print("âŒ æœªæ‰¾åˆ°CSVæ–‡ä»¶")
        return
    
    print(f"ğŸ“ æ‰¾åˆ° {len(csv_files)} ä¸ªCSVæ–‡ä»¶:")
    for i, file in enumerate(csv_files, 1):
        print(f"  {i}. {file.name}")
    
    try:
        choice = int(input("\né€‰æ‹©è¦åˆ†æçš„æ–‡ä»¶ (è¾“å…¥åºå·): "))
        if 1 <= choice <= len(csv_files):
            selected_file = csv_files[choice - 1]
            print(f"ğŸ” æ­£åœ¨åˆ†æ: {selected_file.name}")
            
            result = analyze_comments_from_csv(selected_file)
            if result[0] is not None:
                print("âœ… è¯äº‘åˆ†æå®Œæˆï¼")
                print(f"ğŸ“Š è¯äº‘å›¾ç‰‡å·²ä¿å­˜åˆ°: {DATA_DIR}/wordcloud.png")
            else:
                print("âŒ è¯äº‘åˆ†æå¤±è´¥")
        else:
            print("âŒ æ— æ•ˆé€‰æ‹©")
    except ValueError:
        print("âŒ è¯·è¾“å…¥æœ‰æ•ˆæ•°å­—")

def system_check():
    """ç³»ç»Ÿæ£€æŸ¥"""
    print("ğŸ”§ ç³»ç»Ÿæ£€æŸ¥")
    print("-" * 30)
    
    # æ£€æŸ¥Pythonç‰ˆæœ¬
    print(f"Pythonç‰ˆæœ¬: {sys.version}")
    
    # æ£€æŸ¥å¿…è¦çš„åŒ…
    required_packages = [
        'streamlit', 'pandas', 'requests', 'wordcloud', 
        'jieba', 'streamlit-echarts', 'altair', 'python-dotenv'
    ]
    
    missing_packages = []
    for package in required_packages:
        try:
            __import__(package.replace('-', '_'))
            print(f"âœ… {package}: å·²å®‰è£…")
        except ImportError:
            print(f"âŒ {package}: æœªå®‰è£…")
            missing_packages.append(package)
    
    if missing_packages:
        print(f"\nâš ï¸  ç¼ºå°‘ä»¥ä¸‹åŒ…: {', '.join(missing_packages)}")
        print("è¯·è¿è¡Œ: pip install -r requirements.txt")
    
    # æ£€æŸ¥ç¯å¢ƒå˜é‡
    print(f"\nğŸ”‘ ç¯å¢ƒå˜é‡æ£€æŸ¥:")
    bili_cookie = os.getenv('BILI_COOKIE')
    if bili_cookie:
        print("âœ… BILI_COOKIE: å·²é…ç½®")
    else:
        print("âŒ BILI_COOKIE: æœªé…ç½® (éœ€è¦åœ¨.envæ–‡ä»¶ä¸­è®¾ç½®)")
    
    # æ£€æŸ¥ç›®å½•ç»“æ„
    print(f"\nğŸ“ ç›®å½•ç»“æ„æ£€æŸ¥:")
    required_dirs = ['data', 'config', 'analysis', 'report', 'crawler']
    for dir_name in required_dirs:
        dir_path = project_root / dir_name
        if dir_path.exists():
            print(f"âœ… {dir_name}/: å­˜åœ¨")
        else:
            print(f"âŒ {dir_name}/: ä¸å­˜åœ¨")
    
    print("\nğŸ æ£€æŸ¥å®Œæˆ")

if __name__ == "__main__":
    main()
