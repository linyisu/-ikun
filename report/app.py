import streamlit as st
import pandas as pd
import os
import csv
import time
import json
import hashlib
import urllib
import re
import requests
from datetime import datetime
from dotenv import load_dotenv
import matplotlib.pyplot as plt
from wordcloud import WordCloud
import jieba
from streamlit_echarts import st_echarts
from collections import Counter

st.markdown(
    """
    <style>
    .block-container {
        max-width: 55%;
        margin: 0 auto;
    }
    [data-testid="stImageContainer"] {
        display: flex;
        justify-content: center;
    }
    [data-testid="stImageContainer"] img {
        width: 65% !important;
        height: auto !important;
        border-radius: 16px;
        box-shadow: 0 4px 10px rgba(0,0,0,0.15);
        display: block;
        margin-left: auto;
        margin-right: auto;
    }

    </style>
    """,
    unsafe_allow_html=True
)

# ========== Â∑•ÂÖ∑ÂáΩÊï∞ ==========
def get_Header():
    cookie = os.getenv('BILI_COOKIE')
    header = {
        "Cookie": cookie,
        "User-Agent": 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/134.0.0.0 Safari/537.36'
    }
    return header

def get_avid_from_bv(bv):
    url = f'https://api.bilibili.com/x/web-interface/view?bvid={bv}'
    response = requests.get(url, headers=get_Header())
    data = response.json()
    if data.get('code', 0) != 0:
        raise Exception(f"Ëé∑ÂèñavidÂ§±Ë¥•: {data.get('message', 'Êú™Áü•ÈîôËØØ')}")
    return data['data']['aid']

def md5(code):
    return hashlib.md5(code.encode('utf-8')).hexdigest()

def fetch_comments(bv, is_second=True, max_pages=1000):
    aid = get_avid_from_bv(bv)
    pagination_str = ''
    count = 0
    comments = []

    for page in range(max_pages):
        wts = int(time.time())
        offset = f'"offset":"{pagination_str}"' if pagination_str else '"offset":""'
        payload = f"mode=2&oid={aid}&pagination_str=%7B{offset}%7D&plat=1&type=1&web_location=1315875&wts={wts}"
        w_rid = md5(payload + 'ea1db124af3c7062474693fa704f4ff8')
        url = f"https://api.bilibili.com/x/v2/reply/wbi/main?oid={aid}&type=1&mode=2&pagination_str=%7B{offset}%7D&plat=1&web_location=1315875&w_rid={w_rid}&wts={wts}"

        res = requests.get(url, headers=get_Header()).json()
        replies = res.get("data", {}).get("replies", [])
        if not replies:
            break

        for r in replies:
            count += 1
            comments.append({
                "Áî®Êà∑Âêç": r["member"]["uname"],
                "ËØÑËÆ∫ÂÜÖÂÆπ": r["content"]["message"],
                "ÁÇπËµûÊï∞": r["like"],
                "ËØÑËÆ∫Êó∂Èó¥": datetime.fromtimestamp(r["ctime"]).strftime('%Y-%m-%d %H:%M:%S'),
            })

        pagination_str = res['data']['cursor']['pagination_reply'].get('next_offset', 0)
        if not pagination_str:
            break
        time.sleep(0.5)
    
    return comments

# ========== Streamlit App ==========
def main():
    load_dotenv()
    st.title("üé• BÁ´ôËØÑËÆ∫Áà¨ÂèñÂô®")

    bv = st.text_input("ËØ∑ËæìÂÖ•BVÂè∑:", "")
    is_click = st.button("ÂºÄÂßãÁà¨Âèñ")

    df = None
    file_path = None

    if is_click and bv:
        with st.spinner("Ê≠£Âú®Áà¨ÂèñËØÑËÆ∫‰∏≠..."):
            try:
                data = fetch_comments(bv, max_pages=15)  # Âè™Áà¨15È°µÈò≤Ê≠¢Â§™ÊÖ¢
                # cy
                if not data:
                    st.warning("Ê≤°ÊúâÁà¨ÂèñÂà∞‰ªª‰ΩïËØÑËÆ∫„ÄÇ")
                    return
                df = pd.DataFrame(data)
                st.session_state['df'] = df  # Â≠òÂÖ•session_state
                st.success(f"‚úÖ Áà¨ÂèñÊàêÂäüÔºåÂÖ± {len(df)} Êù°ËØÑËÆ∫")
                # Âè™ÊòæÁ§∫‰∏ÄÊ¨°ËØÑËÆ∫Âå∫Âíå‰∏ãËΩΩÊåâÈíÆ
                with st.container():
                    st.dataframe(df, key="comments_df")
                    file_path = f"data/{bv}_comments.csv"
                    df.to_csv(file_path, index=False, encoding="utf-8-sig")
                    st.session_state['file_path'] = file_path  # Â≠òÂÖ•session_state
                    st.download_button("üì• ‰∏ãËΩΩËØÑËÆ∫ CSV Êñá‰ª∂", data=df.to_csv(index=False, encoding="utf-8-sig"), file_name=f"{bv}_comments.csv", mime="text/csv", key="download_btn")
            except Exception as e:
                st.error(f"‚ùå Âá∫Èîô‰∫ÜÔºö{e}")
                import traceback
                st.text(traceback.format_exc())
                return
    # Âè™Âú®ÊúâËØÑËÆ∫Êï∞ÊçÆÂêéÊâçÊòæÁ§∫ÂàÜÊûêËØç‰∫ëÊåâÈíÆ
    if 'df' in st.session_state and st.session_state['df'] is not None:
        with st.container():
            if st.button("ÂàÜÊûêËØç‰∫ëÔºàÈùôÊÄÅÂõæÁâáÔºâ"):
                st.info("Ê≠£Âú®ÁîüÊàêËØç‰∫ëÔºåËØ∑Á®çÂÄô...")
                df = st.session_state.get('df', None)
                file_path = st.session_state.get('file_path', None)
                if df is None and file_path:
                    df = pd.read_csv(file_path)
                if df is None or 'ËØÑËÆ∫ÂÜÖÂÆπ' not in df.columns:
                    st.error("Ê≤°ÊúâÂèØÁî®ÁöÑËØÑËÆ∫ÂÜÖÂÆπÔºåÊó†Ê≥ïÁîüÊàêËØç‰∫ë„ÄÇ")
                    return
                text = ' '.join(df['ËØÑËÆ∫ÂÜÖÂÆπ'].astype(str))
                stopwords = set()
                try:
                    with open('stopwords.txt', 'r', encoding='utf-8') as f:
                        for line in f:
                            stopwords.add(line.strip())
                except FileNotFoundError:
                    stopwords = set(['ÁöÑ', '‰∫Ü', 'Âíå', 'ÊòØ', 'Êàë', '‰πü', 'Â∞±', 'ÈÉΩ', 'ËÄå', 'Âèä', '‰∏é', 'ÁùÄ', 'Êàñ', '‰∏Ä‰∏™', 'Ê≤°Êúâ', 'Êàë‰ª¨', '‰Ω†', '‰Ω†‰ª¨', '‰ªñ', 'Â•π', 'ÂÆÉ', 'Âïä', 'Âêß', 'Âêó', 'Âë¢'])
                words = [w for w in jieba.cut(text) if w not in stopwords and len(w) > 1]
                if not words:
                    st.warning("ÂàÜËØçÂêéÊó†ÊúâÊïàËØçËØ≠ÔºåÊó†Ê≥ïÁîüÊàêËØç‰∫ë„ÄÇ")
                    return
                result = ' '.join(words)
                wc = WordCloud(font_path='msyh.ttc', background_color='white', width=800, height=600)
                try:
                    wc.generate(result)
                    st.image(wc.to_array(), caption="ËØÑËÆ∫ËØç‰∫ë", use_container_width=True)
                except Exception as e:
                    st.error(f"ËØç‰∫ëÁîüÊàêÂ§±Ë¥•Ôºö{e}")
                    import traceback
                    st.text(traceback.format_exc())
            if st.button("ÂàÜÊûêËØç‰∫ëÔºàÂèØ‰∫§‰∫íÔºâ"):
                df = st.session_state.get('df', None)
                file_path = st.session_state.get('file_path', None)
                if df is None and file_path:
                    df = pd.read_csv(file_path)
                if df is None or 'ËØÑËÆ∫ÂÜÖÂÆπ' not in df.columns:
                    st.error("Ê≤°ÊúâÂèØÁî®ÁöÑËØÑËÆ∫ÂÜÖÂÆπÔºåÊó†Ê≥ïÁîüÊàêËØç‰∫ë„ÄÇ")
                    return
                text = ' '.join(df['ËØÑËÆ∫ÂÜÖÂÆπ'].astype(str))
                stopwords = set()
                try:
                    with open('stopwords.txt', 'r', encoding='utf-8') as f:
                        for line in f:
                            stopwords.add(line.strip())
                except FileNotFoundError:
                    stopwords = set(['ÁöÑ', '‰∫Ü', 'Âíå', 'ÊòØ', 'Êàë', '‰πü', 'Â∞±', 'ÈÉΩ', 'ËÄå', 'Âèä', '‰∏é', 'ÁùÄ', 'Êàñ', '‰∏Ä‰∏™', 'Ê≤°Êúâ', 'Êàë‰ª¨', '‰Ω†', '‰Ω†‰ª¨', '‰ªñ', 'Â•π', 'ÂÆÉ', 'Âïä', 'Âêß', 'Âêó', 'Âë¢'])
                words = [w for w in jieba.cut(text) if w not in stopwords and len(w) > 1]
                if not words:
                    st.warning("ÂàÜËØçÂêéÊó†ÊúâÊïàËØçËØ≠ÔºåÊó†Ê≥ïÁîüÊàêËØç‰∫ë„ÄÇ")
                    return
                word_freq = Counter(words)
                data = [{"name": k, "value": v} for k, v in word_freq.most_common(200)]
                option = {
                    "backgroundColor": "#f5f7fa",  # ÊµÖËâ≤ËÉåÊôØ
                    "tooltip": {"show": True},
                    "series": [{
                        "type": 'wordCloud',
                        "shape": 'star',
                        "gridSize": 8,
                        "sizeRange": [20, 90],
                        "rotationRange": [-45, 90],
                        "rotationStep": 15,
                        "left": "center",
                        "top": "center",
                        "width": "100%",
                        "height": "100%",
                        "textStyle": {
                            "fontFamily": "ÂæÆËΩØÈõÖÈªë",
                            "fontWeight": "bold",
                            "shadowColor": "#b7e3fa",
                            "shadowBlur": 8,
                            "color": {
                                "type": "radial",
                                "x": 0.5,
                                "y": 0.5,
                                "r": 0.8,
                                "colorStops": [
                                    {"offset": 0, "color": "#91caff"},
                                    {"offset": 0.5, "color": "#f9e79f"},
                                    {"offset": 1, "color": "#ffb3b3"}
                                ]
                            }
                        },
                        "emphasis": {
                            "focus": "self",
                            "textStyle": {
                                "shadowBlur": 16,
                                "shadowColor": "#fff",
                                "color": "#faad14"
                            }
                        },
                        "drawOutOfBound": False,
                        "data": data
                    }]
                }
                st_echarts(option, height="600px")

if __name__ == "__main__":
    main()
